{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b9990b",
   "metadata": {},
   "source": [
    "# Simple RAG Chatbot using Ollama & FAISS\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Load External Data**\n",
    "2. **Generate Embeddings**\n",
    "3. **Create Vector Index**\n",
    "4. **Create Retriever**\n",
    "5. **Complete RAG Chat App**\n",
    "    * Create History Aware Retriever\n",
    "    * Create RAG Chat App\n",
    "\n",
    "### Installation\n",
    "\n",
    "* **pip install ollama**\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b809fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "LLM = \"llama2\"\n",
    "\n",
    "response = ollama.generate(model=LLM, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4646547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with a person or product called \"Claude 3.\" Could you please provide more context or information about who or what Claude 3 is? That will help me better understand your question and give you a more accurate response.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08499c86",
   "metadata": {},
   "source": [
    "## 1. Load External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d035680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page-content': 'Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllProductClaude 3 Haiku: our fastest model yet  2 min readProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC',\n",
       " 'metadata': {'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    docs.append({\"page-content\": soup.text, \"metadata\": {\"source\": url}})\n",
    "    \n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29dc8b",
   "metadata": {},
   "source": [
    "## 2. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a376f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = \"llama2\"\n",
    "\n",
    "embeds = ollama.embeddings(model=embedding_model, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "type(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b909ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.039122223854065,\n",
       "  -1.5061262845993042,\n",
       "  0.9997416138648987,\n",
       "  -0.21553783118724823,\n",
       "  -2.3464105129241943],\n",
       " 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds[\"embedding\"][:5], len(embeds[\"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47b816",
   "metadata": {},
   "source": [
    "## 3. Create Vector Index\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167d0311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f64b188edf0> >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dims = 4096\n",
    "\n",
    "vector_index = faiss.IndexFlatL2(dims)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1637b5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs_embeds = []\n",
    "\n",
    "for doc in docs:\n",
    "    resp = ollama.embeddings(model=embedding_model, prompt=doc[\"page-content\"])\n",
    "    docs_embeds.append(resp[\"embedding\"])\n",
    "    \n",
    "vector_index.add(np.array(docs_embeds))\n",
    "\n",
    "vector_index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551ebdd",
   "metadata": {},
   "source": [
    "## 4. Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5474d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query_embeds: list[float], top_k: int=4)-> tuple[np.array, np.array]:\n",
    "    distances, indexes = vector_index.search(query_embeds, top_k)\n",
    "    return distances, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ca0e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[18513.047, 18802.807, 18904.535, 19758.133]], dtype=float32),\n",
       " array([[3, 5, 6, 0]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = ollama.embeddings(model=embedding_model, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "D, I = retriever(np.array(embeds[\"embedding\"]).reshape(1,-1))\n",
    "\n",
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c65a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-3-family'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n",
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n"
     ]
    }
   ],
   "source": [
    "for idx in I[0]:\n",
    "    print(docs[idx][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f27d64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query: str, top_k: int=4)-> list[dict]:\n",
    "    embeds = ollama.embeddings(model=embedding_model, prompt=query)\n",
    "    D, I = retriever(np.array(embeds[\"embedding\"]).reshape(1,-1))\n",
    "    \n",
    "    return [doc for idx, doc in enumerate(docs) if idx in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caaee6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-3-family'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = retrieve_relevant_docs(\"Do you know about Claude 3?\")\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(doc[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef734ea8",
   "metadata": {},
   "source": [
    "## 5. Complete RAG Chat App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf7c6d",
   "metadata": {},
   "source": [
    "### 5.1 Create History Aware Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5651c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_aware_query(query: str, chat_history: list[dict]):\n",
    "    complete_history = chat_history +\\\n",
    "    [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n",
    "    }]\n",
    "    \n",
    "    resp = ollama.chat(model=LLM, messages=complete_history)\n",
    "    \n",
    "    return resp[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cabdc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Based on our conversation, here\\'s a potential search query you could use:\\n\\n\"Claude 3 models: Opus, Haiku, Sonnet - features, differences, and applications\"\\n\\nThis search query should return results that provide detailed information about each of the three models in Claude 3, including their respective features, capabilities, and potential applications.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Do you know about Claude 3?\",\n",
    "},\n",
    "{\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Yes, I am well aware of Claude 3 AI conversational bot from Anthropic which has 3 models (Opus, Haiku & Sonnet). Please provide more context info on how can I help you.\",\n",
    "}]\n",
    "\n",
    "modified_query = create_history_aware_query(\"Tell me about different models in detail.\", chat_history)\n",
    "\n",
    "modified_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ca0c3",
   "metadata": {},
   "source": [
    "### 5.2 Create RAG Chat App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "396a4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query, context):\n",
    "    return f\"\"\"\n",
    "        Answer the following question based on the provided context only.\n",
    "        \n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Question: {query}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e254741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat_app(query: str, chat_history: list[dict])-> str:\n",
    "    \n",
    "    modified_query = create_history_aware_query(query, chat_history)\n",
    "    \n",
    "    relevant_docs = retrieve_relevant_docs(modified_query)\n",
    "\n",
    "    context = \"\\n\".join([doc[\"page-content\"] for doc in docs])\n",
    "    \n",
    "    prompt = create_prompt(query, context)\n",
    "    \n",
    "    messages = chat_history + [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }]\n",
    "    \n",
    "    response = ollama.chat(model=LLM, messages=messages)\n",
    "    \n",
    "    return response[\"message\"][\"content\"], relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2814d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Claude 3 is an AI conversational bot developed by Anthropic, which offers three distinct models: Opus, Haiku, and Sonnet. Here's a detailed overview of each model:\n",
      "\n",
      "1. **Opus**:\n",
      "Opus is the flagship model of Claude 3, designed to generate coherent and contextually relevant text. It can engage in conversation, answer questions, and even create stories or poems. Opus has been trained on a diverse range of texts, including books, articles, and websites, allowing it to understand different writing styles and language nuances.\n",
      "2. **Haiku**:\n",
      "Haiku is a more lighthearted and playful model than Opus. It's designed to generate short, funny, or quirky responses, often with a touch of humor or sarcasm. Haiku can be used for entertainment purposes, such as creating silly chatbot interactions or generating humorous responses to user input.\n",
      "3. **Sonnet**:\n",
      "Sonnet is the most creative and expressive model in Claude 3. It's designed to generate poetic or artistic responses, often with a deeper meaning or emotional resonance. Sonnet can be used for various purposes, such as creating poetry, writing short stories, or even composing music.\n",
      "\n",
      "Each model in Claude 3 has its unique strengths and weaknesses, allowing you to choose the one that best fits your needs and goals. By understanding the different models and their capabilities, you can leverage Claude 3 for a wide range of applications, from entertainment and creative writing to more serious tasks like language translation or text summarization.\n",
      "\n",
      "Please let me know if you have any specific questions or areas of interest regarding Claude 3 and its models.\n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = rag_chat_app(\"Tell me about different models in detail.\", chat_history)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba171ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1-prompting'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(doc[\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10eb8e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Do you know about Claude 3?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Yes, I am well aware of Claude 3 AI conversational bot from Anthropic which has 3 models (Opus, Haiku & Sonnet). Please provide more context info on how can I help you.'},\n",
       " {'role': 'user', 'content': 'Tell me about different models in detail.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Certainly! Claude 3 is an AI conversational bot developed by Anthropic, which offers three distinct models: Opus, Haiku, and Sonnet. Here's a detailed overview of each model:\\n\\n1. **Opus**:\\nOpus is the flagship model of Claude 3, designed to generate coherent and contextually relevant text. It can engage in conversation, answer questions, and even create stories or poems. Opus has been trained on a diverse range of texts, including books, articles, and websites, allowing it to understand different writing styles and language nuances.\\n2. **Haiku**:\\nHaiku is a more lighthearted and playful model than Opus. It's designed to generate short, funny, or quirky responses, often with a touch of humor or sarcasm. Haiku can be used for entertainment purposes, such as creating silly chatbot interactions or generating humorous responses to user input.\\n3. **Sonnet**:\\nSonnet is the most creative and expressive model in Claude 3. It's designed to generate poetic or artistic responses, often with a deeper meaning or emotional resonance. Sonnet can be used for various purposes, such as creating poetry, writing short stories, or even composing music.\\n\\nEach model in Claude 3 has its unique strengths and weaknesses, allowing you to choose the one that best fits your needs and goals. By understanding the different models and their capabilities, you can leverage Claude 3 for a wide range of applications, from entertainment and creative writing to more serious tasks like language translation or text summarization.\\n\\nPlease let me know if you have any specific questions or areas of interest regarding Claude 3 and its models.\"}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Tell me about different models in detail.\"\n",
    "})\n",
    "\n",
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": answer\n",
    "})\n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f79381c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Claude 3 Opus is the flagship model of Claude 3, designed to generate coherent and contextually relevant text. Here are some key features and capabilities of Opus:\n",
      "\n",
      "1. **Contextual understanding**:\n",
      "Opus has been trained on a diverse range of texts, including books, articles, and websites. This training allows it to understand different writing styles, language nuances, and cultural references. As a result, Opus can engage in conversation that is both relevant and contextually appropriate.\n",
      "2. **Natural language processing**:\n",
      "Opus has been fine-tuned using advanced natural language processing techniques. This enables it to generate text that is not only grammatically correct but also sounds like it was written by a human. Opus can understand the nuances of language, such as tone, style, and syntax, making its responses feel more natural and human-like.\n",
      "3. **Creative generation**:\n",
      "Opus is capable of generating text that goes beyond simple answers or responses. It can create stories, poems, or even entire articles based on a given prompt or topic. This makes Opus a versatile tool for writers, content creators, and anyone looking to generate creative content quickly and efficiently.\n",
      "4. **Emotional intelligence**:\n",
      "Opus has been trained on a wide range of texts, including those with emotional content. As a result, it can recognize and respond to emotions in a way that feels natural and empathetic. This makes Opus an excellent conversational partner for anyone looking to engage in meaningful and emotionally intelligent interactions.\n",
      "5. **Customization**:\n",
      "Claude 3 offers a range of customization options, allowing you to tailor Opus's responses to your specific needs and preferences. You can adjust the tone, style, and language use to suit your context and audience.\n",
      "6. **Integration with other tools**:\n",
      "Claude 3 is designed to integrate seamlessly with other tools and platforms. For example, you can use Opus to generate content for blogs, social media, or chatbots. You can also integrate Opus into your own applications or workflows using the API or SDK provided by Anthropic.\n",
      "\n",
      "Overall, Claude 3 Opus is a powerful tool for generating coherent and contextually relevant text. Its ability to understand context, recognize emotions, and generate creative content makes it an excellent conversational partner for a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = rag_chat_app(\"Tell me more about Claude 3 Opus.\", chat_history)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fc2dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1-prompting'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(doc[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f116b9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this video, I explained how to create simple **RAG** chatbot application using **Ollama** & **FAISS**. Feel free to let me know your views and doubts in comments section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b8625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

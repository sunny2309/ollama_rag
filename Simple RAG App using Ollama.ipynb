{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb155be",
   "metadata": {},
   "source": [
    "# Simple RAG App using Ollama & FAISS\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Load External Data**\n",
    "2. **Generate Embeddings**\n",
    "3. **Create Vector Index**\n",
    "4. **Create Retriever**\n",
    "5. **RAG App**\n",
    "\n",
    "### Installation\n",
    "\n",
    "* **pip install ollama**\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7412ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "LLM = \"llama2\"\n",
    "\n",
    "response = ollama.generate(model=LLM, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee362f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I apologize, but I'm a large language model, I don't have access to information about a person or entity called \"Claude 3.\" Could you please provide more context or clarify who or what Claude 3 refers to?\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259d490",
   "metadata": {},
   "source": [
    "## 1. Load External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c21cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page-content': 'Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllProductClaude 3 Haiku: our fastest model yet  2 min readProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC',\n",
       " 'metadata': {'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    docs.append({\"page-content\": soup.text, \"metadata\": {\"source\": url}})\n",
    "    \n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b561ee",
   "metadata": {},
   "source": [
    "## 2. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e53c9625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = \"llama2\"\n",
    "\n",
    "embeds = ollama.embeddings(model=embedding_model, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "type(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "adebf45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.039122223854065,\n",
       "  -1.5061262845993042,\n",
       "  0.9997416138648987,\n",
       "  -0.21553783118724823,\n",
       "  -2.3464105129241943],\n",
       " 4096)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds[\"embedding\"][:5], len(embeds[\"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43945079",
   "metadata": {},
   "source": [
    "## 3. Create Vector Index\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d24d29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f2635a8aa00> >"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dims = 4096\n",
    "\n",
    "vector_index = faiss.IndexFlatL2(dims)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53b2e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs_embeds = []\n",
    "\n",
    "for doc in docs:\n",
    "    resp = ollama.embeddings(model=embedding_model, prompt=doc[\"page-content\"])\n",
    "    docs_embeds.append(resp[\"embedding\"])\n",
    "    \n",
    "vector_index.add(np.array(docs_embeds))\n",
    "\n",
    "vector_index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f80c90",
   "metadata": {},
   "source": [
    "## 4. Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "125a5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query_embeds: list[float], top_k: int=4)-> tuple[np.array, np.array]:\n",
    "    distances, indexes = vector_index.search(query_embeds, top_k)\n",
    "    return distances, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2607aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[18513.047, 18802.807, 18904.535, 19758.133]], dtype=float32),\n",
       " array([[3, 5, 6, 0]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = ollama.embeddings(model=embedding_model, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "D, I = retriever(np.array(embeds[\"embedding\"]).reshape(1,-1))\n",
    "\n",
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bc05cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-3-family'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n",
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n"
     ]
    }
   ],
   "source": [
    "for idx in I[0]:\n",
    "    print(docs[idx][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d60a4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query: str, top_k: int=4)-> list[dict]:\n",
    "    embeds = ollama.embeddings(model=embedding_model, prompt=query)\n",
    "    D, I = retriever(np.array(embeds[\"embedding\"]).reshape(1,-1))\n",
    "    \n",
    "    return [doc for idx, doc in enumerate(docs) if idx in I[0]] ## To DO: Loop through Indexes instead of Docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "778b2562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-2-1'}\n",
      "{'source': 'https://www.anthropic.com/news/claude-3-family'}\n",
      "{'source': 'https://www.anthropic.com/claude'}\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = retrieve_relevant_docs(\"Do you know about Claude 3?\")\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(doc[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb6135",
   "metadata": {},
   "source": [
    "## 5. Complete RAG App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c651803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query: str, context: str)-> str:\n",
    "    return f\"\"\"\n",
    "        Answer the following question based on the provided context only.\n",
    "        \n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Question: {query}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92a018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_app(query: str)-> str:\n",
    "    \n",
    "    relevant_docs = retrieve_relevant_docs(query)\n",
    "\n",
    "    context = \"\\n\".join([doc[\"page-content\"] for doc in relevant_docs])\n",
    "    \n",
    "    prompt = create_prompt(query, context)\n",
    "    \n",
    "    response = ollama.generate(model=LLM, prompt=prompt)\n",
    "    \n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5981b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Yes, I am familiar with Claude 3. It is a family of foundational AI models that can be used in various applications such as customer interactions, content moderation, and cost-saving tasks. Claude 3 consists of three models: Haiku, Sonnet, and Opus, each with its own unique capabilities and strengths.\n",
      "\n",
      "Claude 3 offers several advantages over other AI models on the market, including faster execution, lower costs, and improved intelligence. The models are also designed to be secure, accessible, and trustworthy, making them ideal for enterprise use cases.\n",
      "\n",
      "Some of the key features of Claude 3 include:\n",
      "\n",
      "* Advanced reasoning capabilities: Claude 3 can perform complex cognitive tasks beyond simple pattern recognition or text generation.\n",
      "* Vision analysis: Transcribe and analyze almost any static image, from handwritten notes and graphs to photographs.\n",
      "* Code generation: Start creating websites in HTML and CSS, turning images into structured JSON data, or debugging complex code bases.\n",
      "* Multilingual processing: Translate between various languages in real-time, practice grammar, or create multi-lingual content.\n",
      "* Right-sized for any task: The Claude 3 family of models offers the best combination of speed and performance for enterprise use cases, at a lower cost than other models on the market.\n",
      "\n",
      "Overall, Claude 3 is designed to provide fast, capable, and truly conversational AI experiences that can help businesses transform their operations and customer interactions.\n"
     ]
    }
   ],
   "source": [
    "response = rag_app(\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136f6d5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this video, I explained how to create simple **RAG** application using LLMs available through **Ollama**. Feel free to let me know your views and doubts in comments section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689ba55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
